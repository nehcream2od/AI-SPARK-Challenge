{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"epochs\": 3000,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-6,\n",
    "    \"generator_hidden_size\": [16, 8],\n",
    "    \"discriminator_hidden_size\": [8],\n",
    "    \"alpha\": 0.9,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    # torch.backends.cudnn.deterministic = True  # type: ignore # 속도 이슈\n",
    "    # torch.backends.cudnn.benchmark = False  # type: ignore # 속도 이슈\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle\n",
    "with open(\"train_list.pkl\", \"rb\") as f:\n",
    "    train_list = pickle.load(f)\n",
    "with open(\"test_list.pkl\", \"rb\") as f:\n",
    "    test_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_list)):\n",
    "#     train_list[i] = train_list[i] * 10\n",
    "#     test_list[i] = test_list[i] * 10\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):  # 초기 데이터 생성 방법을 지정\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):  # 데이터의 전체 길이\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.data = self.dataset[idx]\n",
    "        return torch.tensor(self.data, dtype=torch.float32).to(device)\n",
    "        # \"y\":torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[0])\n",
    "        self.fc4 = nn.Linear(hidden_size[0], output_size)\n",
    "        self.layer_norm_hidden_0 = nn.LayerNorm(hidden_size[0])\n",
    "        self.layer_norm_hidden_1 = nn.LayerNorm(hidden_size[1])\n",
    "        self.rrelu = nn.RReLU()\n",
    "        self.drop = nn.Dropout(0.0)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rrelu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.layer_norm_hidden_0(x)\n",
    "        x = self.rrelu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.layer_norm_hidden_1(x)\n",
    "        x = self.rrelu(self.fc3(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.layer_norm_hidden_0(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        for layer in module.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], output_size)\n",
    "        self.layer_norm_hidden_0 = nn.LayerNorm(hidden_size[0])\n",
    "        self.rrelu = nn.RReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.drop = nn.Dropout(0.0)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rrelu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.layer_norm_hidden_0(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        for layer in module.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = self.loss_fn.reduction  # mean, sum, etc..\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        bceloss = self.loss_fn(pred, true)\n",
    "\n",
    "        pred_prob = torch.sigmoid(\n",
    "            pred\n",
    "        )  # p  pt는 p가 true 이면 pt = p / false 이면 pt = 1 - p\n",
    "        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)  # add balance\n",
    "        modulating_factor = torch.abs(true - pred_prob) ** self.gamma  # focal term\n",
    "        loss = alpha_factor * modulating_factor * bceloss  # bceloss에 이미 음수가 들어가 있음\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "\n",
    "        else:  # 'none'\n",
    "            return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "        self, patience=5, delta=0, checkpoint_path=\"checkpoint.pth\", device=\"cpu\"\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.device = device\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        # score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(\n",
    "            {\"model_state_dict\": model.state_dict(), \"val_loss\": val_loss},\n",
    "            self.checkpoint_path,\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        return checkpoint[\"val_loss\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosineannealingwarmrestarts|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(\n",
    "        self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1.0, last_epoch=-1\n",
    "    ):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [\n",
    "                (self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                base_lr\n",
    "                + (self.eta_max - base_lr)\n",
    "                * (\n",
    "                    1\n",
    "                    + math.cos(\n",
    "                        math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up)\n",
    "                    )\n",
    "                )\n",
    "                / 2\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(\n",
    "                        math.log(\n",
    "                            (epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult\n",
    "                        )\n",
    "                    )\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult**n - 1) / (\n",
    "                        self.T_mult - 1\n",
    "                    )\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group[\"lr\"] = lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lion(Optimizer):\n",
    "    r\"\"\"Implements Lion algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-4,\n",
    "        betas=(0.9, 0.99),\n",
    "        weight_decay=0.0,\n",
    "        maximize=False,\n",
    "        foreach=None,\n",
    "    ):\n",
    "        \"\"\"Initialize the hyperparameters.\n",
    "        Args:\n",
    "          params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "          lr (float, optional): learning rate (default: 1e-4)\n",
    "          betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.99))\n",
    "          weight_decay (float, optional): weight decay coefficient (default: 0)\n",
    "        \"\"\"\n",
    "\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            weight_decay=weight_decay,\n",
    "            foreach=foreach,\n",
    "            maximize=maximize,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"maximize\", False)\n",
    "            group.setdefault(\"foreach\", None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "          closure (callable, optional): A closure that reevaluates the model\n",
    "            and returns the loss.\n",
    "        Returns:\n",
    "          the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                params_with_grad.append(p)\n",
    "                if p.grad.is_sparse:\n",
    "                    raise RuntimeError(\"Lion does not support sparse gradients\")\n",
    "                grads.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "\n",
    "                exp_avgs.append(state[\"exp_avg\"])\n",
    "\n",
    "            lion(\n",
    "                params_with_grad,\n",
    "                grads,\n",
    "                exp_avgs,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                lr=group[\"lr\"],\n",
    "                weight_decay=group[\"weight_decay\"],\n",
    "                maximize=group[\"maximize\"],\n",
    "                foreach=group[\"foreach\"],\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def lion(\n",
    "    params: List[torch.Tensor],\n",
    "    grads: List[torch.Tensor],\n",
    "    exp_avgs: List[torch.Tensor],\n",
    "    # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "    # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "    maximize: bool = False,\n",
    "    foreach: bool = None,\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "):\n",
    "    r\"\"\"Functional API that performs Lion algorithm computation.\"\"\"\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError(\"torch.jit.script not supported with foreach optimizers\")\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_lion\n",
    "    else:\n",
    "        func = _single_tensor_lion\n",
    "\n",
    "    func(\n",
    "        params,\n",
    "        grads,\n",
    "        exp_avgs,\n",
    "        beta1=beta1,\n",
    "        beta2=beta2,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        maximize=maximize,\n",
    "    )\n",
    "\n",
    "\n",
    "def _single_tensor_lion(\n",
    "    params: List[torch.Tensor],\n",
    "    grads: List[torch.Tensor],\n",
    "    exp_avgs: List[torch.Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    maximize: bool,\n",
    "):\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i] if not maximize else -grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "\n",
    "        if torch.is_complex(param):\n",
    "            grad = torch.view_as_real(grad)\n",
    "            exp_avg = torch.view_as_real(exp_avg)\n",
    "            param = torch.view_as_real(param)\n",
    "\n",
    "        # Perform stepweight decay\n",
    "        param.mul_(1 - lr * weight_decay)\n",
    "\n",
    "        # Weight update\n",
    "        update = exp_avg.mul(beta1).add_(grad, alpha=1 - beta1)\n",
    "        param.add_(torch.sign(update), alpha=-lr)\n",
    "\n",
    "        # Decay the momentum running average coefficient\n",
    "        exp_avg.lerp_(grad, 1 - beta2)\n",
    "\n",
    "\n",
    "def _multi_tensor_lion(\n",
    "    params: List[torch.Tensor],\n",
    "    grads: List[torch.Tensor],\n",
    "    exp_avgs: List[torch.Tensor],\n",
    "    *,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    maximize: bool,\n",
    "):\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    if maximize:\n",
    "        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]\n",
    "\n",
    "    grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in grads]\n",
    "    exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avgs]\n",
    "    params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in params]\n",
    "\n",
    "    # Perform stepweight decay\n",
    "    torch._foreach_mul_(params, 1 - lr * weight_decay)\n",
    "\n",
    "    # Weight update\n",
    "    updates = torch._foreach_mul(exp_avgs, beta1)\n",
    "    torch._foreach_add_(updates, grads, alpha=1 - beta1)\n",
    "\n",
    "    updates = [u.sign() for u in updates]\n",
    "    torch._foreach_add_(params, updates, alpha=-lr)\n",
    "\n",
    "    # Decay the momentum running average coefficient\n",
    "    torch._foreach_mul_(exp_avgs, beta2)\n",
    "    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(torch.initial_seed() % (2**32 - 1))\n",
    "\n",
    "\n",
    "def train_gan(\n",
    "    generator, discriminator, data, epochs=1000, batch_size=32, lr=1e-6, alpha=0.5\n",
    "):\n",
    "    # optimizers\n",
    "    # optimizer_G = Lion(generator.parameters(), lr=lr)\n",
    "    # optimizer_D = Lion(discriminator.parameters(), lr=lr)\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    # loss function\n",
    "    criterion_G = F.mse_loss\n",
    "    # criterion_D = FocalLoss()\n",
    "    criterion_D = F.binary_cross_entropy_with_logits\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=200, checkpoint_path=\"model_checkpoint.pth\", device=device\n",
    "    )\n",
    "\n",
    "    # lr scheduler\n",
    "    scheduler_G = CosineAnnealingWarmUpRestarts(\n",
    "        optimizer_G, T_0=100, T_mult=2, eta_max=0.01, T_up=10, gamma=0.9\n",
    "    )\n",
    "    scheduler_D = CosineAnnealingWarmUpRestarts(\n",
    "        optimizer_D, T_0=100, T_mult=2, eta_max=0.1, T_up=10, gamma=0.9\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        dataset = CustomDataset(data)\n",
    "        train_size = int(len(dataset) * 0.8)\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        train_g_loss = 0.0\n",
    "        train_d_loss = 0.0\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for inputs in train_dataloader:\n",
    "            # train discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            batch_size = inputs.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            real_outputs = discriminator(inputs)\n",
    "            real_loss = criterion_D(real_outputs, real_labels)\n",
    "\n",
    "            fake_inputs = generator(inputs)\n",
    "            fake_outputs = discriminator(fake_inputs.detach())\n",
    "            fake_loss = criterion_D(fake_outputs, fake_labels)\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            train_d_loss += d_loss.item()\n",
    "\n",
    "            # train generator\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_outputs = discriminator(fake_inputs)\n",
    "\n",
    "            # generator가 생성한 가짜 데이터와 실제 데이터 사이의 차이를 최소화, 생성된 가짜 데이터에 대해 discriminator가 1을 출력한 결과를 실제 레이블과 비교\n",
    "            g_loss = criterion_G(fake_inputs, inputs) * alpha + criterion_D(\n",
    "                fake_outputs, real_labels\n",
    "            ) * (1 - alpha)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            train_g_loss += g_loss.item()\n",
    "\n",
    "        train_d_loss /= len(train_dataloader)\n",
    "        train_g_loss /= len(train_dataloader)\n",
    "\n",
    "        # Validate\n",
    "        with torch.no_grad():\n",
    "            val_g_loss = 0.0\n",
    "            val_d_loss = 0.0\n",
    "\n",
    "            generator.eval()\n",
    "            discriminator.eval()\n",
    "\n",
    "            for inputs in val_dataloader:\n",
    "                batch_size = inputs.size(0)\n",
    "                real_labels = torch.ones(batch_size, 1).to(device)\n",
    "                fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "                real_outputs = discriminator(inputs)\n",
    "                real_loss = criterion_D(real_outputs, real_labels)\n",
    "\n",
    "                fake_inputs = generator(inputs)\n",
    "                fake_outputs = discriminator(fake_inputs.detach())\n",
    "                fake_loss = criterion_D(fake_outputs, fake_labels)\n",
    "\n",
    "                d_loss = real_loss + fake_loss\n",
    "                val_d_loss += d_loss.item()\n",
    "\n",
    "                fake_outputs = discriminator(fake_inputs)\n",
    "                g_loss = criterion_G(fake_inputs, inputs) * alpha + criterion_D(\n",
    "                    fake_outputs, real_labels\n",
    "                ) * (1 - alpha)\n",
    "                val_g_loss += g_loss.item()\n",
    "\n",
    "            val_d_loss /= len(val_dataloader)\n",
    "            val_g_loss /= len(val_dataloader)\n",
    "\n",
    "        # lr scheduler step\n",
    "        scheduler_G.step(epoch)\n",
    "        scheduler_D.step(epoch)\n",
    "\n",
    "        # early stopping 호출\n",
    "        early_stopping(val_g_loss, generator)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            # load the last checkpoint with the best model\n",
    "            early_stopping.load_checkpoint(generator)\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs} | Train D Loss: {train_d_loss:.4f} | Train G Loss: {train_g_loss:.4f} | Val D Loss: {val_d_loss:.4f} | Val G Loss: {val_g_loss:.4f}\"\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(generator, data):\n",
    "    prediction = []\n",
    "    dataset = CustomDataset(data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        worker_init_fn=worker_init_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            output = generator(inputs).float()\n",
    "            prediction.append(output.numpy())\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 번째 설비 학습 & 예측 시작\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.4246 | Train G Loss: 0.1554 | Val D Loss: 2.6554 | Val G Loss: 0.2181\n",
      "Epoch 20/3000 | Train D Loss: 1.3882 | Train G Loss: 0.1104 | Val D Loss: 1.8236 | Val G Loss: 0.1318\n",
      "Epoch 30/3000 | Train D Loss: 1.4280 | Train G Loss: 0.0780 | Val D Loss: 1.4143 | Val G Loss: 0.0664\n",
      "Epoch 40/3000 | Train D Loss: 1.3873 | Train G Loss: 0.0728 | Val D Loss: 1.3873 | Val G Loss: 0.0732\n",
      "Epoch 50/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0705 | Val D Loss: 1.3869 | Val G Loss: 0.0695\n",
      "Epoch 60/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0701 | Val D Loss: 1.3866 | Val G Loss: 0.0715\n",
      "Epoch 70/3000 | Train D Loss: 1.3857 | Train G Loss: 0.0698 | Val D Loss: 1.3906 | Val G Loss: 0.0689\n",
      "Epoch 80/3000 | Train D Loss: 1.3855 | Train G Loss: 0.0696 | Val D Loss: 1.3885 | Val G Loss: 0.0694\n",
      "Epoch 90/3000 | Train D Loss: 1.3869 | Train G Loss: 0.0695 | Val D Loss: 1.3872 | Val G Loss: 0.0695\n",
      "Epoch 100/3000 | Train D Loss: 1.3853 | Train G Loss: 0.0696 | Val D Loss: 1.3867 | Val G Loss: 0.0694\n",
      "Epoch 110/3000 | Train D Loss: 1.3943 | Train G Loss: 0.0715 | Val D Loss: 1.3874 | Val G Loss: 0.0713\n",
      "Epoch 120/3000 | Train D Loss: 1.3872 | Train G Loss: 0.0706 | Val D Loss: 1.3872 | Val G Loss: 0.0692\n",
      "Epoch 130/3000 | Train D Loss: 1.4275 | Train G Loss: 0.0975 | Val D Loss: 1.3999 | Val G Loss: 0.0662\n",
      "Epoch 140/3000 | Train D Loss: 1.3875 | Train G Loss: 0.0706 | Val D Loss: 1.3867 | Val G Loss: 0.0695\n",
      "Epoch 150/3000 | Train D Loss: 1.3887 | Train G Loss: 0.0702 | Val D Loss: 1.3919 | Val G Loss: 0.0726\n",
      "Epoch 160/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0708 | Val D Loss: 1.3864 | Val G Loss: 0.0696\n",
      "Epoch 170/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 180/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0699\n",
      "Epoch 190/3000 | Train D Loss: 1.3468 | Train G Loss: 0.0748 | Val D Loss: 1.3817 | Val G Loss: 0.0739\n",
      "Epoch 200/3000 | Train D Loss: 1.3881 | Train G Loss: 0.0769 | Val D Loss: 1.5929 | Val G Loss: 0.0685\n",
      "Epoch 210/3000 | Train D Loss: 1.3552 | Train G Loss: 0.1140 | Val D Loss: 1.6205 | Val G Loss: 0.0933\n",
      "Epoch 220/3000 | Train D Loss: 1.3619 | Train G Loss: 0.0831 | Val D Loss: 1.0656 | Val G Loss: 0.1286\n",
      "Epoch 230/3000 | Train D Loss: 1.2946 | Train G Loss: 0.0950 | Val D Loss: 0.9722 | Val G Loss: 0.1453\n",
      "Epoch 240/3000 | Train D Loss: 1.3279 | Train G Loss: 0.0915 | Val D Loss: 1.2216 | Val G Loss: 0.0653\n",
      "Epoch 250/3000 | Train D Loss: 1.3300 | Train G Loss: 0.0861 | Val D Loss: 1.0901 | Val G Loss: 0.1104\n",
      "Epoch 260/3000 | Train D Loss: 1.2731 | Train G Loss: 0.0912 | Val D Loss: 0.8858 | Val G Loss: 0.1207\n",
      "Epoch 270/3000 | Train D Loss: 1.3223 | Train G Loss: 0.0840 | Val D Loss: 1.1234 | Val G Loss: 0.1056\n",
      "Epoch 280/3000 | Train D Loss: 1.3451 | Train G Loss: 0.0778 | Val D Loss: 1.2821 | Val G Loss: 0.0804\n",
      "Epoch 290/3000 | Train D Loss: 1.3919 | Train G Loss: 0.0702 | Val D Loss: 1.3989 | Val G Loss: 0.0710\n",
      "Epoch 300/3000 | Train D Loss: 1.4081 | Train G Loss: 0.0696 | Val D Loss: 1.3980 | Val G Loss: 0.0691\n",
      "Epoch 310/3000 | Train D Loss: 1.2658 | Train G Loss: 0.1234 | Val D Loss: 0.9736 | Val G Loss: 0.0890\n",
      "Epoch 320/3000 | Train D Loss: 1.3045 | Train G Loss: 0.1193 | Val D Loss: 1.1854 | Val G Loss: 0.0938\n",
      "Epoch 330/3000 | Train D Loss: 1.2561 | Train G Loss: 0.1282 | Val D Loss: 1.0486 | Val G Loss: 0.0870\n",
      "Epoch 340/3000 | Train D Loss: 1.1631 | Train G Loss: 0.1205 | Val D Loss: 1.1595 | Val G Loss: 0.1046\n",
      "Epoch 350/3000 | Train D Loss: 1.2753 | Train G Loss: 0.1272 | Val D Loss: 1.2965 | Val G Loss: 0.0657\n",
      "Epoch 360/3000 | Train D Loss: 1.3910 | Train G Loss: 0.0715 | Val D Loss: 1.3760 | Val G Loss: 0.0722\n",
      "Epoch 370/3000 | Train D Loss: 1.0026 | Train G Loss: 0.2317 | Val D Loss: 0.6499 | Val G Loss: 0.2825\n",
      "Epoch 380/3000 | Train D Loss: 1.1640 | Train G Loss: 0.1507 | Val D Loss: 0.9333 | Val G Loss: 0.2619\n",
      "Epoch 390/3000 | Train D Loss: 1.2830 | Train G Loss: 0.0865 | Val D Loss: 0.9170 | Val G Loss: 0.2135\n",
      "Epoch 400/3000 | Train D Loss: 1.2313 | Train G Loss: 0.1122 | Val D Loss: 1.2136 | Val G Loss: 0.1215\n",
      "Epoch 410/3000 | Train D Loss: 1.1190 | Train G Loss: 0.1476 | Val D Loss: 1.3141 | Val G Loss: 0.0513\n",
      "Epoch 420/3000 | Train D Loss: 1.3371 | Train G Loss: 0.1013 | Val D Loss: 1.0523 | Val G Loss: 0.0909\n",
      "Epoch 430/3000 | Train D Loss: 1.1644 | Train G Loss: 0.1738 | Val D Loss: 0.5162 | Val G Loss: 0.3395\n",
      "Epoch 440/3000 | Train D Loss: 1.3980 | Train G Loss: 0.1949 | Val D Loss: 1.3831 | Val G Loss: 0.0685\n",
      "Epoch 450/3000 | Train D Loss: 1.2018 | Train G Loss: 0.1355 | Val D Loss: 0.4605 | Val G Loss: 0.3512\n",
      "Epoch 460/3000 | Train D Loss: 1.0396 | Train G Loss: 0.2059 | Val D Loss: 1.4131 | Val G Loss: 0.1056\n",
      "Epoch 470/3000 | Train D Loss: 1.1500 | Train G Loss: 0.1905 | Val D Loss: 1.3925 | Val G Loss: 0.0940\n",
      "Epoch 480/3000 | Train D Loss: 1.1802 | Train G Loss: 0.1220 | Val D Loss: 1.1742 | Val G Loss: 0.1828\n",
      "Epoch 490/3000 | Train D Loss: 1.0599 | Train G Loss: 0.2250 | Val D Loss: 0.9555 | Val G Loss: 0.1270\n",
      "Epoch 500/3000 | Train D Loss: 1.0819 | Train G Loss: 0.1486 | Val D Loss: 1.1786 | Val G Loss: 0.0848\n",
      "Epoch 510/3000 | Train D Loss: 1.1274 | Train G Loss: 0.1819 | Val D Loss: 1.2114 | Val G Loss: 0.0998\n",
      "Epoch 520/3000 | Train D Loss: 1.1106 | Train G Loss: 0.1680 | Val D Loss: 1.0164 | Val G Loss: 0.1985\n",
      "Epoch 530/3000 | Train D Loss: 1.1317 | Train G Loss: 0.1312 | Val D Loss: 0.5457 | Val G Loss: 0.2565\n",
      "Epoch 540/3000 | Train D Loss: 1.0632 | Train G Loss: 0.2147 | Val D Loss: 1.1964 | Val G Loss: 0.0962\n",
      "Epoch 550/3000 | Train D Loss: 1.0338 | Train G Loss: 0.1476 | Val D Loss: 1.1879 | Val G Loss: 0.1162\n",
      "Epoch 560/3000 | Train D Loss: 1.1440 | Train G Loss: 0.1143 | Val D Loss: 1.0734 | Val G Loss: 0.1727\n",
      "Epoch 570/3000 | Train D Loss: 1.0007 | Train G Loss: 0.1951 | Val D Loss: 1.0717 | Val G Loss: 0.1205\n",
      "Epoch 580/3000 | Train D Loss: 1.1359 | Train G Loss: 0.1141 | Val D Loss: 0.7784 | Val G Loss: 0.1261\n",
      "Epoch 590/3000 | Train D Loss: 1.0894 | Train G Loss: 0.1244 | Val D Loss: 1.1357 | Val G Loss: 0.0933\n",
      "Epoch 600/3000 | Train D Loss: 1.1583 | Train G Loss: 0.1885 | Val D Loss: 1.0562 | Val G Loss: 0.0930\n",
      "Epoch 610/3000 | Train D Loss: 0.9220 | Train G Loss: 0.3399 | Val D Loss: 1.4020 | Val G Loss: 0.2483\n",
      "Epoch 620/3000 | Train D Loss: 1.1978 | Train G Loss: 0.1341 | Val D Loss: 1.2232 | Val G Loss: 0.0785\n",
      "Epoch 630/3000 | Train D Loss: 1.1824 | Train G Loss: 0.1220 | Val D Loss: 1.3843 | Val G Loss: 0.0905\n",
      "Epoch 640/3000 | Train D Loss: 1.2749 | Train G Loss: 0.0989 | Val D Loss: 1.4804 | Val G Loss: 0.0688\n",
      "Epoch 650/3000 | Train D Loss: 1.3553 | Train G Loss: 0.0806 | Val D Loss: 1.4531 | Val G Loss: 0.0725\n",
      "Epoch 660/3000 | Train D Loss: 1.4147 | Train G Loss: 0.0691 | Val D Loss: 1.4199 | Val G Loss: 0.0678\n",
      "Epoch 670/3000 | Train D Loss: 1.3930 | Train G Loss: 0.0699 | Val D Loss: 1.3882 | Val G Loss: 0.0695\n",
      "Epoch 680/3000 | Train D Loss: 1.3961 | Train G Loss: 0.0698 | Val D Loss: 1.3887 | Val G Loss: 0.0685\n",
      "Epoch 690/3000 | Train D Loss: 1.3917 | Train G Loss: 0.0697 | Val D Loss: 1.3860 | Val G Loss: 0.0692\n",
      "Epoch 700/3000 | Train D Loss: 1.3888 | Train G Loss: 0.0696 | Val D Loss: 1.3884 | Val G Loss: 0.0695\n",
      "Epoch 710/3000 | Train D Loss: 1.1609 | Train G Loss: 0.1521 | Val D Loss: 0.6858 | Val G Loss: 0.2594\n",
      "Epoch 720/3000 | Train D Loss: 1.0273 | Train G Loss: 0.1913 | Val D Loss: 0.7813 | Val G Loss: 0.1613\n",
      "Epoch 730/3000 | Train D Loss: 0.9605 | Train G Loss: 0.2071 | Val D Loss: 0.7653 | Val G Loss: 0.1422\n",
      "Early stopping\n",
      "0 번째 설비의 threshold: 0.019985645351544856\n",
      "\n",
      "1 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/h6r1lsxx1k1gcpbs79bpj3m80000gn/T/ipykernel_42368/1884697218.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.data, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/3000 | Train D Loss: 1.5706 | Train G Loss: 0.1362 | Val D Loss: 1.5642 | Val G Loss: 0.1057\n",
      "Epoch 20/3000 | Train D Loss: 1.3862 | Train G Loss: 0.0850 | Val D Loss: 1.3830 | Val G Loss: 0.0745\n",
      "Epoch 30/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0762 | Val D Loss: 1.3870 | Val G Loss: 0.0758\n",
      "Epoch 40/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0780 | Val D Loss: 1.3863 | Val G Loss: 0.0781\n",
      "Epoch 50/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0734 | Val D Loss: 1.3863 | Val G Loss: 0.0728\n",
      "Epoch 60/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0714 | Val D Loss: 1.3863 | Val G Loss: 0.0703\n",
      "Epoch 70/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0705 | Val D Loss: 1.3863 | Val G Loss: 0.0698\n",
      "Epoch 80/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0700 | Val D Loss: 1.3863 | Val G Loss: 0.0699\n",
      "Epoch 90/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 100/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 110/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0837 | Val D Loss: 1.3876 | Val G Loss: 0.0823\n",
      "Epoch 120/3000 | Train D Loss: 1.3934 | Train G Loss: 0.0720 | Val D Loss: 1.3902 | Val G Loss: 0.0660\n",
      "Epoch 130/3000 | Train D Loss: 1.3923 | Train G Loss: 0.0766 | Val D Loss: 1.3883 | Val G Loss: 0.0712\n",
      "Epoch 140/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0717 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 150/3000 | Train D Loss: 1.3870 | Train G Loss: 0.0714 | Val D Loss: 1.3863 | Val G Loss: 0.0713\n",
      "Epoch 160/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0708 | Val D Loss: 1.3863 | Val G Loss: 0.0702\n",
      "Epoch 170/3000 | Train D Loss: 1.3888 | Train G Loss: 0.0710 | Val D Loss: 1.3861 | Val G Loss: 0.0711\n",
      "Epoch 180/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0707 | Val D Loss: 1.3863 | Val G Loss: 0.0703\n",
      "Epoch 190/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0712 | Val D Loss: 1.3870 | Val G Loss: 0.0712\n",
      "Epoch 200/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0705 | Val D Loss: 1.3863 | Val G Loss: 0.0701\n",
      "Epoch 210/3000 | Train D Loss: 1.3876 | Train G Loss: 0.0706 | Val D Loss: 1.3882 | Val G Loss: 0.0684\n",
      "Epoch 220/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0701 | Val D Loss: 1.3863 | Val G Loss: 0.0690\n",
      "Epoch 230/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 240/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0698\n",
      "Epoch 250/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 260/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 270/3000 | Train D Loss: 1.3856 | Train G Loss: 0.0697 | Val D Loss: 1.3864 | Val G Loss: 0.0694\n",
      "Epoch 280/3000 | Train D Loss: 1.3857 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 290/3000 | Train D Loss: 1.3851 | Train G Loss: 0.0696 | Val D Loss: 1.3864 | Val G Loss: 0.0695\n",
      "Epoch 300/3000 | Train D Loss: 1.3847 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 310/3000 | Train D Loss: 1.3952 | Train G Loss: 0.0722 | Val D Loss: 1.3864 | Val G Loss: 0.0698\n",
      "Early stopping\n",
      "1 번째 설비의 threshold: 0.03400961979764965\n",
      "\n",
      "2 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.7426 | Train G Loss: 0.1622 | Val D Loss: 2.4945 | Val G Loss: 0.1939\n",
      "Epoch 20/3000 | Train D Loss: 1.3880 | Train G Loss: 0.0723 | Val D Loss: 1.3865 | Val G Loss: 0.0727\n",
      "Epoch 30/3000 | Train D Loss: 1.3869 | Train G Loss: 0.0744 | Val D Loss: 1.3869 | Val G Loss: 0.0697\n",
      "Epoch 40/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0746 | Val D Loss: 1.3863 | Val G Loss: 0.0706\n",
      "Epoch 50/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0707 | Val D Loss: 1.3863 | Val G Loss: 0.0704\n",
      "Epoch 60/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0703 | Val D Loss: 1.3863 | Val G Loss: 0.0698\n",
      "Epoch 70/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0700 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 80/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0700\n",
      "Epoch 90/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 100/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 110/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0767 | Val D Loss: 1.3870 | Val G Loss: 0.0704\n",
      "Epoch 120/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0783 | Val D Loss: 1.3864 | Val G Loss: 0.0709\n",
      "Epoch 130/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0714 | Val D Loss: 1.3863 | Val G Loss: 0.0698\n",
      "Epoch 140/3000 | Train D Loss: 1.3893 | Train G Loss: 0.0717 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 150/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0716 | Val D Loss: 1.3863 | Val G Loss: 0.0703\n",
      "Epoch 160/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0715 | Val D Loss: 1.3866 | Val G Loss: 0.0718\n",
      "Epoch 170/3000 | Train D Loss: 1.3900 | Train G Loss: 0.0716 | Val D Loss: 1.3863 | Val G Loss: 0.0708\n",
      "Epoch 180/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0701 | Val D Loss: 1.3870 | Val G Loss: 0.0724\n",
      "Epoch 190/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0704 | Val D Loss: 1.3863 | Val G Loss: 0.0708\n",
      "Epoch 200/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 210/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0701 | Val D Loss: 1.3888 | Val G Loss: 0.0652\n",
      "Epoch 220/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0702\n",
      "Epoch 230/3000 | Train D Loss: 1.3861 | Train G Loss: 0.0697 | Val D Loss: 1.3865 | Val G Loss: 0.0698\n",
      "Epoch 240/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 250/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 260/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 270/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 280/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 290/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 300/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 310/3000 | Train D Loss: 1.3884 | Train G Loss: 0.0712 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 320/3000 | Train D Loss: 1.3861 | Train G Loss: 0.0705 | Val D Loss: 1.3862 | Val G Loss: 0.0682\n",
      "Epoch 330/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0705 | Val D Loss: 1.3873 | Val G Loss: 0.0732\n",
      "Epoch 340/3000 | Train D Loss: 1.3883 | Train G Loss: 0.0702 | Val D Loss: 1.4023 | Val G Loss: 0.0857\n",
      "Epoch 350/3000 | Train D Loss: 1.3887 | Train G Loss: 0.0709 | Val D Loss: 1.3918 | Val G Loss: 0.0759\n",
      "Early stopping\n",
      "2 번째 설비의 threshold: 0.010079569075499053\n",
      "\n",
      "3 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.3879 | Train G Loss: 0.0821 | Val D Loss: 1.3886 | Val G Loss: 0.0717\n",
      "Epoch 20/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0735 | Val D Loss: 1.3863 | Val G Loss: 0.0768\n",
      "Epoch 30/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0748 | Val D Loss: 1.3863 | Val G Loss: 0.0714\n",
      "Epoch 40/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0716 | Val D Loss: 1.3863 | Val G Loss: 0.0730\n",
      "Epoch 50/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0709 | Val D Loss: 1.3864 | Val G Loss: 0.0703\n",
      "Epoch 60/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0704 | Val D Loss: 1.3863 | Val G Loss: 0.0714\n",
      "Epoch 70/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0702\n",
      "Epoch 80/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 90/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 100/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 110/3000 | Train D Loss: 1.3923 | Train G Loss: 0.0713 | Val D Loss: 1.3864 | Val G Loss: 0.0705\n",
      "Epoch 120/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0726 | Val D Loss: 1.3863 | Val G Loss: 0.0705\n",
      "Epoch 130/3000 | Train D Loss: 1.3898 | Train G Loss: 0.0709 | Val D Loss: 1.3863 | Val G Loss: 0.0699\n",
      "Epoch 140/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0719 | Val D Loss: 1.3863 | Val G Loss: 0.0711\n",
      "Epoch 150/3000 | Train D Loss: 1.3869 | Train G Loss: 0.0714 | Val D Loss: 1.3863 | Val G Loss: 0.0688\n",
      "Epoch 160/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0744 | Val D Loss: 1.3863 | Val G Loss: 0.0708\n",
      "Epoch 170/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0723 | Val D Loss: 1.3862 | Val G Loss: 0.0724\n",
      "Epoch 180/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0707 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 190/3000 | Train D Loss: 1.3886 | Train G Loss: 0.0701 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 200/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0701 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 210/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3864 | Val G Loss: 0.0690\n",
      "Epoch 220/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 230/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0698 | Val D Loss: 1.3864 | Val G Loss: 0.0708\n",
      "Epoch 240/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0698\n",
      "Epoch 250/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 260/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 270/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 280/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 290/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 300/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 310/3000 | Train D Loss: 1.3900 | Train G Loss: 0.0752 | Val D Loss: 1.3884 | Val G Loss: 0.0770\n",
      "Epoch 320/3000 | Train D Loss: 1.3872 | Train G Loss: 0.0718 | Val D Loss: 1.3874 | Val G Loss: 0.0746\n",
      "Epoch 330/3000 | Train D Loss: 1.3870 | Train G Loss: 0.0703 | Val D Loss: 1.3865 | Val G Loss: 0.0697\n",
      "Epoch 340/3000 | Train D Loss: 1.3870 | Train G Loss: 0.0697 | Val D Loss: 1.3865 | Val G Loss: 0.0691\n",
      "Epoch 350/3000 | Train D Loss: 1.3876 | Train G Loss: 0.0706 | Val D Loss: 1.3875 | Val G Loss: 0.0714\n",
      "Epoch 360/3000 | Train D Loss: 1.3883 | Train G Loss: 0.0703 | Val D Loss: 1.3867 | Val G Loss: 0.0678\n",
      "Early stopping\n",
      "3 번째 설비의 threshold: 0.0018197210888226411\n",
      "\n",
      "4 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.4140 | Train G Loss: 0.0926 | Val D Loss: 1.3904 | Val G Loss: 0.0890\n",
      "Epoch 20/3000 | Train D Loss: 1.3899 | Train G Loss: 0.0725 | Val D Loss: 1.3867 | Val G Loss: 0.0707\n",
      "Epoch 30/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0731 | Val D Loss: 1.3863 | Val G Loss: 0.0705\n",
      "Epoch 40/3000 | Train D Loss: 1.3874 | Train G Loss: 0.0741 | Val D Loss: 1.3914 | Val G Loss: 0.0668\n",
      "Epoch 50/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0714 | Val D Loss: 1.3860 | Val G Loss: 0.0719\n",
      "Epoch 60/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0711 | Val D Loss: 1.3863 | Val G Loss: 0.0700\n",
      "Epoch 70/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 80/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 90/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 100/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 110/3000 | Train D Loss: 1.3869 | Train G Loss: 0.0731 | Val D Loss: 1.3864 | Val G Loss: 0.0727\n",
      "Epoch 120/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0743 | Val D Loss: 1.3867 | Val G Loss: 0.0827\n",
      "Epoch 130/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0753 | Val D Loss: 1.3868 | Val G Loss: 0.0722\n",
      "Epoch 140/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0718 | Val D Loss: 1.3863 | Val G Loss: 0.0702\n",
      "Epoch 150/3000 | Train D Loss: 1.3885 | Train G Loss: 0.0729 | Val D Loss: 1.3870 | Val G Loss: 0.0712\n",
      "Epoch 160/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0713 | Val D Loss: 1.3853 | Val G Loss: 0.0710\n",
      "Epoch 170/3000 | Train D Loss: 1.3888 | Train G Loss: 0.0702 | Val D Loss: 1.3876 | Val G Loss: 0.0673\n",
      "Epoch 180/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 190/3000 | Train D Loss: 1.3872 | Train G Loss: 0.0712 | Val D Loss: 1.3862 | Val G Loss: 0.0707\n",
      "Epoch 200/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0698 | Val D Loss: 1.3864 | Val G Loss: 0.0703\n",
      "Epoch 210/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0701 | Val D Loss: 1.3863 | Val G Loss: 0.0701\n",
      "Epoch 220/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 230/3000 | Train D Loss: 1.3871 | Train G Loss: 0.0695 | Val D Loss: 1.3865 | Val G Loss: 0.0679\n",
      "Epoch 240/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 250/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 260/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 270/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 280/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0689\n",
      "Epoch 290/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 300/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 310/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0713 | Val D Loss: 1.3869 | Val G Loss: 0.0713\n",
      "Epoch 320/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0700 | Val D Loss: 1.3868 | Val G Loss: 0.0705\n",
      "Epoch 330/3000 | Train D Loss: 1.3962 | Train G Loss: 0.0726 | Val D Loss: 1.3864 | Val G Loss: 0.0693\n",
      "Epoch 340/3000 | Train D Loss: 1.3873 | Train G Loss: 0.0704 | Val D Loss: 1.3877 | Val G Loss: 0.0661\n",
      "Epoch 350/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 360/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0701 | Val D Loss: 1.3874 | Val G Loss: 0.0679\n",
      "Epoch 370/3000 | Train D Loss: 1.3880 | Train G Loss: 0.0708 | Val D Loss: 1.3892 | Val G Loss: 0.0654\n",
      "Epoch 380/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0700 | Val D Loss: 1.3864 | Val G Loss: 0.0699\n",
      "Epoch 390/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0707 | Val D Loss: 1.3863 | Val G Loss: 0.0700\n",
      "Epoch 400/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 410/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0700 | Val D Loss: 1.3865 | Val G Loss: 0.0709\n",
      "Epoch 420/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 430/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0699 | Val D Loss: 1.3872 | Val G Loss: 0.0724\n",
      "Epoch 440/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0700 | Val D Loss: 1.3863 | Val G Loss: 0.0711\n",
      "Epoch 450/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 460/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0700 | Val D Loss: 1.3863 | Val G Loss: 0.0700\n",
      "Epoch 470/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0698 | Val D Loss: 1.3867 | Val G Loss: 0.0686\n",
      "Epoch 480/3000 | Train D Loss: 1.3871 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0695\n",
      "Epoch 490/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0695 | Val D Loss: 1.3873 | Val G Loss: 0.0725\n",
      "Epoch 500/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0699 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 510/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0698 | Val D Loss: 1.3870 | Val G Loss: 0.0704\n",
      "Epoch 520/3000 | Train D Loss: 1.3862 | Train G Loss: 0.0695 | Val D Loss: 1.3866 | Val G Loss: 0.0704\n",
      "Epoch 530/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 540/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0694 | Val D Loss: 1.3867 | Val G Loss: 0.0677\n",
      "Epoch 550/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0697\n",
      "Epoch 560/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 570/3000 | Train D Loss: 1.3858 | Train G Loss: 0.0696 | Val D Loss: 1.3865 | Val G Loss: 0.0708\n",
      "Epoch 580/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 590/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0695 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 600/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 610/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 620/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 630/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 640/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 650/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0693 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 660/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0693 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 670/3000 | Train D Loss: 1.3862 | Train G Loss: 0.0694 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 680/3000 | Train D Loss: 1.3860 | Train G Loss: 0.0693 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 690/3000 | Train D Loss: 1.3867 | Train G Loss: 0.0693 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 700/3000 | Train D Loss: 1.3862 | Train G Loss: 0.0693 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Early stopping\n",
      "4 번째 설비의 threshold: 0.0030867339082269016\n",
      "\n",
      "5 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.5458 | Train G Loss: 0.2137 | Val D Loss: 1.3351 | Val G Loss: 0.0906\n",
      "Epoch 20/3000 | Train D Loss: 1.3992 | Train G Loss: 0.0842 | Val D Loss: 1.3544 | Val G Loss: 0.0807\n",
      "Epoch 30/3000 | Train D Loss: 1.3822 | Train G Loss: 0.0782 | Val D Loss: 1.3479 | Val G Loss: 0.0693\n",
      "Epoch 40/3000 | Train D Loss: 1.3912 | Train G Loss: 0.0733 | Val D Loss: 1.4056 | Val G Loss: 0.0682\n",
      "Epoch 50/3000 | Train D Loss: 1.2952 | Train G Loss: 0.1360 | Val D Loss: 1.3467 | Val G Loss: 0.1636\n",
      "Epoch 60/3000 | Train D Loss: 1.3811 | Train G Loss: 0.0732 | Val D Loss: 1.3604 | Val G Loss: 0.0792\n",
      "Epoch 70/3000 | Train D Loss: 1.3775 | Train G Loss: 0.0756 | Val D Loss: 1.3915 | Val G Loss: 0.0717\n",
      "Epoch 80/3000 | Train D Loss: 1.3805 | Train G Loss: 0.0704 | Val D Loss: 1.3707 | Val G Loss: 0.0720\n",
      "Epoch 90/3000 | Train D Loss: 1.3936 | Train G Loss: 0.0693 | Val D Loss: 1.3801 | Val G Loss: 0.0698\n",
      "Epoch 100/3000 | Train D Loss: 1.3848 | Train G Loss: 0.0699 | Val D Loss: 1.3859 | Val G Loss: 0.0691\n",
      "Epoch 110/3000 | Train D Loss: 1.2942 | Train G Loss: 0.1150 | Val D Loss: 2.1562 | Val G Loss: 0.0709\n",
      "Epoch 120/3000 | Train D Loss: 1.2782 | Train G Loss: 0.1353 | Val D Loss: 1.0767 | Val G Loss: 0.1294\n",
      "Epoch 130/3000 | Train D Loss: 1.3453 | Train G Loss: 0.0919 | Val D Loss: 1.3461 | Val G Loss: 0.0982\n",
      "Epoch 140/3000 | Train D Loss: 1.2666 | Train G Loss: 0.1168 | Val D Loss: 2.1460 | Val G Loss: 0.0452\n",
      "Epoch 150/3000 | Train D Loss: 1.3008 | Train G Loss: 0.0973 | Val D Loss: 1.1851 | Val G Loss: 0.0548\n",
      "Epoch 160/3000 | Train D Loss: 1.2416 | Train G Loss: 0.1594 | Val D Loss: 1.4999 | Val G Loss: 0.1072\n",
      "Epoch 170/3000 | Train D Loss: 1.3286 | Train G Loss: 0.1043 | Val D Loss: 1.1093 | Val G Loss: 0.1158\n",
      "Epoch 180/3000 | Train D Loss: 1.3085 | Train G Loss: 0.0957 | Val D Loss: 1.2565 | Val G Loss: 0.0701\n",
      "Epoch 190/3000 | Train D Loss: 1.3360 | Train G Loss: 0.0775 | Val D Loss: 1.2973 | Val G Loss: 0.0766\n",
      "Epoch 200/3000 | Train D Loss: 1.2794 | Train G Loss: 0.1179 | Val D Loss: 1.1519 | Val G Loss: 0.1268\n",
      "Epoch 210/3000 | Train D Loss: 1.1274 | Train G Loss: 0.1723 | Val D Loss: 1.2264 | Val G Loss: 0.0833\n",
      "Epoch 220/3000 | Train D Loss: 1.3124 | Train G Loss: 0.0970 | Val D Loss: 1.8261 | Val G Loss: 0.0330\n",
      "Epoch 230/3000 | Train D Loss: 1.2898 | Train G Loss: 0.0856 | Val D Loss: 1.2173 | Val G Loss: 0.0806\n",
      "Epoch 240/3000 | Train D Loss: 1.3165 | Train G Loss: 0.0870 | Val D Loss: 1.3328 | Val G Loss: 0.0734\n",
      "Epoch 250/3000 | Train D Loss: 1.1593 | Train G Loss: 0.1353 | Val D Loss: 1.2614 | Val G Loss: 0.0761\n",
      "Epoch 260/3000 | Train D Loss: 1.0319 | Train G Loss: 0.1634 | Val D Loss: 1.1024 | Val G Loss: 0.1414\n",
      "Epoch 270/3000 | Train D Loss: 1.3114 | Train G Loss: 0.0817 | Val D Loss: 1.2105 | Val G Loss: 0.0912\n",
      "Epoch 280/3000 | Train D Loss: 1.4078 | Train G Loss: 0.0713 | Val D Loss: 1.3876 | Val G Loss: 0.0736\n",
      "Epoch 290/3000 | Train D Loss: 1.3937 | Train G Loss: 0.0720 | Val D Loss: 1.3883 | Val G Loss: 0.0701\n",
      "Epoch 300/3000 | Train D Loss: 1.3925 | Train G Loss: 0.0711 | Val D Loss: 1.3862 | Val G Loss: 0.0695\n",
      "Epoch 310/3000 | Train D Loss: 1.2913 | Train G Loss: 0.1030 | Val D Loss: 1.3067 | Val G Loss: 0.0887\n",
      "Epoch 320/3000 | Train D Loss: 1.2949 | Train G Loss: 0.1262 | Val D Loss: 1.2287 | Val G Loss: 0.0741\n",
      "Epoch 330/3000 | Train D Loss: 1.2293 | Train G Loss: 0.0992 | Val D Loss: 0.8920 | Val G Loss: 0.2211\n",
      "Epoch 340/3000 | Train D Loss: 1.1993 | Train G Loss: 0.1854 | Val D Loss: 1.2833 | Val G Loss: 0.1275\n",
      "Epoch 350/3000 | Train D Loss: 1.1313 | Train G Loss: 0.1615 | Val D Loss: 1.6637 | Val G Loss: 0.0584\n",
      "Epoch 360/3000 | Train D Loss: 1.1897 | Train G Loss: 0.1581 | Val D Loss: 0.8913 | Val G Loss: 0.2958\n",
      "Epoch 370/3000 | Train D Loss: 1.0804 | Train G Loss: 0.1881 | Val D Loss: 1.2933 | Val G Loss: 0.1458\n",
      "Epoch 380/3000 | Train D Loss: 1.3003 | Train G Loss: 0.1001 | Val D Loss: 0.9695 | Val G Loss: 0.1043\n",
      "Epoch 390/3000 | Train D Loss: 1.0226 | Train G Loss: 0.2572 | Val D Loss: 0.6803 | Val G Loss: 0.1598\n",
      "Epoch 400/3000 | Train D Loss: 0.7848 | Train G Loss: 0.3718 | Val D Loss: 0.7852 | Val G Loss: 0.6817\n",
      "Epoch 410/3000 | Train D Loss: 1.2192 | Train G Loss: 0.1752 | Val D Loss: 1.3449 | Val G Loss: 0.1277\n",
      "Epoch 420/3000 | Train D Loss: 1.1283 | Train G Loss: 0.1445 | Val D Loss: 0.6441 | Val G Loss: 0.2835\n",
      "Epoch 430/3000 | Train D Loss: 0.9647 | Train G Loss: 0.2235 | Val D Loss: 0.5939 | Val G Loss: 0.3880\n",
      "Epoch 440/3000 | Train D Loss: 1.0948 | Train G Loss: 0.2315 | Val D Loss: 1.3229 | Val G Loss: 0.1132\n",
      "Epoch 450/3000 | Train D Loss: 0.9542 | Train G Loss: 0.2588 | Val D Loss: 0.8413 | Val G Loss: 0.4960\n",
      "Epoch 460/3000 | Train D Loss: 0.2638 | Train G Loss: 0.8252 | Val D Loss: 0.0055 | Val G Loss: 1.0369\n",
      "Epoch 470/3000 | Train D Loss: 1.0387 | Train G Loss: 0.1973 | Val D Loss: 2.2073 | Val G Loss: 0.2380\n",
      "Epoch 480/3000 | Train D Loss: 0.8949 | Train G Loss: 0.3013 | Val D Loss: 0.6424 | Val G Loss: 0.4377\n",
      "Epoch 490/3000 | Train D Loss: 1.1385 | Train G Loss: 0.2075 | Val D Loss: 1.0405 | Val G Loss: 0.1110\n",
      "Epoch 500/3000 | Train D Loss: 0.8889 | Train G Loss: 0.2792 | Val D Loss: 0.4869 | Val G Loss: 0.6176\n",
      "Epoch 510/3000 | Train D Loss: 0.7816 | Train G Loss: 0.5662 | Val D Loss: 0.9982 | Val G Loss: 0.2050\n",
      "Epoch 520/3000 | Train D Loss: 0.9877 | Train G Loss: 0.1864 | Val D Loss: 0.8864 | Val G Loss: 0.1262\n",
      "Epoch 530/3000 | Train D Loss: 1.0850 | Train G Loss: 0.1701 | Val D Loss: 1.0757 | Val G Loss: 0.3320\n",
      "Early stopping\n",
      "5 번째 설비의 threshold: 0.039270686952526744\n",
      "\n",
      "6 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.6071 | Train G Loss: 0.1337 | Val D Loss: 1.3885 | Val G Loss: 0.0766\n",
      "Epoch 20/3000 | Train D Loss: 1.3908 | Train G Loss: 0.0759 | Val D Loss: 1.3971 | Val G Loss: 0.0728\n",
      "Epoch 30/3000 | Train D Loss: 1.3890 | Train G Loss: 0.0729 | Val D Loss: 1.3877 | Val G Loss: 0.0703\n",
      "Epoch 40/3000 | Train D Loss: 1.3871 | Train G Loss: 0.0723 | Val D Loss: 1.3867 | Val G Loss: 0.0689\n",
      "Epoch 50/3000 | Train D Loss: 1.3869 | Train G Loss: 0.0712 | Val D Loss: 1.3867 | Val G Loss: 0.0698\n",
      "Epoch 60/3000 | Train D Loss: 1.3868 | Train G Loss: 0.0703 | Val D Loss: 1.3865 | Val G Loss: 0.0748\n",
      "Epoch 70/3000 | Train D Loss: 1.3874 | Train G Loss: 0.0701 | Val D Loss: 1.3866 | Val G Loss: 0.0697\n",
      "Epoch 80/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0698 | Val D Loss: 1.3865 | Val G Loss: 0.0691\n",
      "Epoch 90/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0694\n",
      "Epoch 100/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0696 | Val D Loss: 1.3863 | Val G Loss: 0.0693\n",
      "Epoch 110/3000 | Train D Loss: 1.3865 | Train G Loss: 0.0736 | Val D Loss: 1.3895 | Val G Loss: 0.0686\n",
      "Epoch 120/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0784 | Val D Loss: 1.3863 | Val G Loss: 0.0748\n",
      "Epoch 130/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0708 | Val D Loss: 1.3863 | Val G Loss: 0.0699\n",
      "Epoch 140/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0705 | Val D Loss: 1.3866 | Val G Loss: 0.0708\n",
      "Epoch 150/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0716 | Val D Loss: 1.3864 | Val G Loss: 0.0714\n",
      "Epoch 160/3000 | Train D Loss: 1.3893 | Train G Loss: 0.0724 | Val D Loss: 1.3865 | Val G Loss: 0.0702\n",
      "Epoch 170/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0697 | Val D Loss: 1.3863 | Val G Loss: 0.0696\n",
      "Epoch 180/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0705\n",
      "Epoch 190/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0709 | Val D Loss: 1.3863 | Val G Loss: 0.0710\n",
      "Epoch 200/3000 | Train D Loss: 1.3863 | Train G Loss: 0.0708 | Val D Loss: 1.3863 | Val G Loss: 0.0731\n",
      "Epoch 210/3000 | Train D Loss: 1.3866 | Train G Loss: 0.0702 | Val D Loss: 1.3860 | Val G Loss: 0.0696\n",
      "Epoch 220/3000 | Train D Loss: 1.3864 | Train G Loss: 0.0698 | Val D Loss: 1.3863 | Val G Loss: 0.0708\n",
      "Early stopping\n",
      "6 번째 설비의 threshold: 0.051625512093909434\n",
      "\n",
      "7 번째 설비 학습 & 예측 시작\n",
      "delete generator completed\n",
      "delete discriminator completed\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 16]             128\n",
      "             RReLU-2                   [-1, 16]               0\n",
      "           Dropout-3                   [-1, 16]               0\n",
      "         LayerNorm-4                   [-1, 16]              32\n",
      "            Linear-5                    [-1, 8]             136\n",
      "             RReLU-6                    [-1, 8]               0\n",
      "           Dropout-7                    [-1, 8]               0\n",
      "         LayerNorm-8                    [-1, 8]              16\n",
      "            Linear-9                   [-1, 16]             144\n",
      "            RReLU-10                   [-1, 16]               0\n",
      "          Dropout-11                   [-1, 16]               0\n",
      "        LayerNorm-12                   [-1, 16]              32\n",
      "           Linear-13                    [-1, 7]             119\n",
      "================================================================\n",
      "Total params: 607\n",
      "Trainable params: 607\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/3000 | Train D Loss: 1.2458 | Train G Loss: 0.1916 | Val D Loss: 1.3110 | Val G Loss: 0.2282\n",
      "Epoch 20/3000 | Train D Loss: 1.2840 | Train G Loss: 0.1143 | Val D Loss: 1.0766 | Val G Loss: 0.1591\n",
      "Epoch 30/3000 | Train D Loss: 1.4497 | Train G Loss: 0.1521 | Val D Loss: 1.6138 | Val G Loss: 0.2030\n",
      "Epoch 40/3000 | Train D Loss: 1.3900 | Train G Loss: 0.0779 | Val D Loss: 1.3303 | Val G Loss: 0.0928\n",
      "Epoch 50/3000 | Train D Loss: 1.3655 | Train G Loss: 0.0803 | Val D Loss: 1.1515 | Val G Loss: 0.1007\n",
      "Epoch 60/3000 | Train D Loss: 1.3412 | Train G Loss: 0.0862 | Val D Loss: 1.3665 | Val G Loss: 0.0824\n",
      "Epoch 70/3000 | Train D Loss: 1.3708 | Train G Loss: 0.0756 | Val D Loss: 1.4288 | Val G Loss: 0.0632\n",
      "Epoch 80/3000 | Train D Loss: 1.3796 | Train G Loss: 0.0710 | Val D Loss: 1.4132 | Val G Loss: 0.0656\n",
      "Epoch 90/3000 | Train D Loss: 1.3675 | Train G Loss: 0.0707 | Val D Loss: 1.3885 | Val G Loss: 0.0701\n",
      "Epoch 100/3000 | Train D Loss: 1.3842 | Train G Loss: 0.0705 | Val D Loss: 1.3767 | Val G Loss: 0.0703\n",
      "Epoch 110/3000 | Train D Loss: 1.3506 | Train G Loss: 0.1729 | Val D Loss: 1.3993 | Val G Loss: 0.1858\n",
      "Epoch 120/3000 | Train D Loss: 1.3697 | Train G Loss: 0.0778 | Val D Loss: 1.3452 | Val G Loss: 0.0671\n",
      "Epoch 130/3000 | Train D Loss: 1.2965 | Train G Loss: 0.0983 | Val D Loss: 1.4243 | Val G Loss: 0.1633\n",
      "Epoch 140/3000 | Train D Loss: 1.2551 | Train G Loss: 0.1045 | Val D Loss: 1.4643 | Val G Loss: 0.0639\n",
      "Epoch 150/3000 | Train D Loss: 1.4409 | Train G Loss: 0.0991 | Val D Loss: 1.3603 | Val G Loss: 0.0848\n",
      "Epoch 160/3000 | Train D Loss: 1.3211 | Train G Loss: 0.0904 | Val D Loss: 1.3589 | Val G Loss: 0.0713\n",
      "Epoch 170/3000 | Train D Loss: 1.2025 | Train G Loss: 0.1447 | Val D Loss: 0.9283 | Val G Loss: 0.1494\n",
      "Epoch 180/3000 | Train D Loss: 1.1565 | Train G Loss: 0.1392 | Val D Loss: 1.1570 | Val G Loss: 0.1134\n",
      "Epoch 190/3000 | Train D Loss: 1.3829 | Train G Loss: 0.0958 | Val D Loss: 1.3759 | Val G Loss: 0.0723\n",
      "Epoch 200/3000 | Train D Loss: 1.1970 | Train G Loss: 0.1339 | Val D Loss: 1.3472 | Val G Loss: 0.0996\n",
      "Epoch 210/3000 | Train D Loss: 1.2600 | Train G Loss: 0.1162 | Val D Loss: 0.9331 | Val G Loss: 0.1921\n",
      "Epoch 220/3000 | Train D Loss: 1.1570 | Train G Loss: 0.1143 | Val D Loss: 2.1920 | Val G Loss: 0.0880\n",
      "Epoch 230/3000 | Train D Loss: 1.3352 | Train G Loss: 0.0836 | Val D Loss: 1.3497 | Val G Loss: 0.0720\n",
      "Epoch 240/3000 | Train D Loss: 1.3164 | Train G Loss: 0.0818 | Val D Loss: 1.1630 | Val G Loss: 0.1010\n",
      "Epoch 250/3000 | Train D Loss: 1.3415 | Train G Loss: 0.0824 | Val D Loss: 1.4215 | Val G Loss: 0.0908\n",
      "Epoch 260/3000 | Train D Loss: 1.3378 | Train G Loss: 0.0792 | Val D Loss: 1.5542 | Val G Loss: 0.0661\n",
      "Epoch 270/3000 | Train D Loss: 1.3603 | Train G Loss: 0.0836 | Val D Loss: 1.2585 | Val G Loss: 0.0841\n",
      "Epoch 280/3000 | Train D Loss: 1.3950 | Train G Loss: 0.0711 | Val D Loss: 1.3968 | Val G Loss: 0.0690\n",
      "Epoch 290/3000 | Train D Loss: 1.3782 | Train G Loss: 0.0703 | Val D Loss: 1.3858 | Val G Loss: 0.0682\n",
      "Epoch 300/3000 | Train D Loss: 1.3745 | Train G Loss: 0.0698 | Val D Loss: 1.3888 | Val G Loss: 0.0698\n",
      "Epoch 310/3000 | Train D Loss: 1.1228 | Train G Loss: 0.1968 | Val D Loss: 1.4393 | Val G Loss: 0.2346\n",
      "Epoch 320/3000 | Train D Loss: 1.2084 | Train G Loss: 0.1284 | Val D Loss: 1.5093 | Val G Loss: 0.0863\n",
      "Epoch 330/3000 | Train D Loss: 1.1851 | Train G Loss: 0.1816 | Val D Loss: 1.2464 | Val G Loss: 0.1680\n",
      "Epoch 340/3000 | Train D Loss: 0.9617 | Train G Loss: 0.2524 | Val D Loss: 1.6795 | Val G Loss: 0.0595\n",
      "Epoch 350/3000 | Train D Loss: 1.2058 | Train G Loss: 0.1258 | Val D Loss: 1.1084 | Val G Loss: 0.1005\n",
      "Epoch 360/3000 | Train D Loss: 1.0238 | Train G Loss: 0.2717 | Val D Loss: 1.2401 | Val G Loss: 0.1224\n",
      "Epoch 370/3000 | Train D Loss: 1.2084 | Train G Loss: 0.1380 | Val D Loss: 1.1381 | Val G Loss: 0.1264\n",
      "Epoch 380/3000 | Train D Loss: 1.1752 | Train G Loss: 0.1471 | Val D Loss: 0.9179 | Val G Loss: 0.2081\n",
      "Early stopping\n",
      "7 번째 설비의 threshold: 0.08324204768116016\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    print()\n",
    "    print(f\"{i} 번째 설비 학습 & 예측 시작\")\n",
    "\n",
    "    # generator and discriminator이라는 변수명이 존재하면 삭제\n",
    "    if \"generator\" in globals():\n",
    "        del generator\n",
    "        print(\"delete generator completed\")\n",
    "    if \"discriminator\" in globals():\n",
    "        del discriminator\n",
    "        print(\"delete discriminator completed\")\n",
    "\n",
    "    input_size = train_list[i].shape[1]\n",
    "    output_size = train_list[i].shape[1]\n",
    "\n",
    "    generator = Generator(input_size, CFG[\"generator_hidden_size\"], output_size)\n",
    "    discriminator = Discriminator(output_size, CFG[\"discriminator_hidden_size\"], 1)\n",
    "    summary(generator, (input_size,))\n",
    "    generator.to(device)\n",
    "\n",
    "    # train\n",
    "    train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        data=train_list[i],\n",
    "        epochs=CFG[\"epochs\"],\n",
    "        batch_size=CFG[\"batch_size\"],\n",
    "        lr=CFG[\"lr\"],\n",
    "        alpha=CFG[\"alpha\"],\n",
    "    )\n",
    "\n",
    "    # set threshold MSE\n",
    "    threshold = np.max(\n",
    "        np.mean(\n",
    "            np.power(\n",
    "                train_list[i]\n",
    "                - np.array(predict(generator, CustomDataset(train_list[i]))).squeeze(1),\n",
    "                2,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    # set threshold MAE\n",
    "    # threshold = np.max(np.mean(np.abs(train_list[i] - np.array(predict(generator, CustomDataset(train_list[i]))).squeeze(1)), axis=1))\n",
    "    print(f\"{i} 번째 설비의 threshold:\", f\"{threshold}\")\n",
    "\n",
    "    # predict MSE\n",
    "    predicted_label = np.where(\n",
    "        np.mean(\n",
    "            np.power(\n",
    "                test_list[i]\n",
    "                - np.array(predict(generator, CustomDataset(test_list[i]))).squeeze(1),\n",
    "                2,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        > threshold,\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "    # predict MAE\n",
    "    # predicted_label = np.where(np.mean(np.abs(test_list[i] - np.array(predict(generator, CustomDataset(test_list[i]))).squeeze(1)), axis=1) > threshold, 1, 0)\n",
    "    predicted_labels.append(predicted_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7032\n",
      "1     357\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type  label\n",
       "0        0      0\n",
       "1        0      0\n",
       "2        0      0\n",
       "3        0      0\n",
       "4        0      0\n",
       "...    ...    ...\n",
       "7384     7      0\n",
       "7385     7      0\n",
       "7386     7      0\n",
       "7387     7      0\n",
       "7388     7      0\n",
       "\n",
       "[7389 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "subminssion = pd.read_csv(\"./dataset/answer_sample.csv\")\n",
    "subminssion[\"label\"] = list(chain(*predicted_labels))\n",
    "subminssion.to_csv(\"answer.csv\", index=False)\n",
    "print(subminssion[\"label\"].value_counts())\n",
    "display(subminssion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7342</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7360</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type  label\n",
       "585      0      1\n",
       "586      0      1\n",
       "587      0      1\n",
       "588      0      1\n",
       "589      0      1\n",
       "...    ...    ...\n",
       "7342     7      1\n",
       "7348     7      1\n",
       "7355     7      1\n",
       "7359     7      1\n",
       "7360     7      1\n",
       "\n",
       "[351 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subminssion_sota = pd.read_csv(\n",
    "    \"sota/robust_8_4_RReLU_layernorm_henormal_lion_lrsche_etamax001_batch16_95점.csv\"\n",
    ")\n",
    "display(subminssion_sota[subminssion_sota[\"label\"] == 1])\n",
    "len(\n",
    "    [\n",
    "        i\n",
    "        for i in subminssion_sota[subminssion_sota[\"label\"] == 1].index\n",
    "        if i in subminssion[subminssion[\"label\"] == 1].index\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
