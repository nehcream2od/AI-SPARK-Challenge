GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name          | Type          | Params
------------------------------------------------
0 | generator     | Generator     | 575
1 | discriminator | Discriminator | 89
------------------------------------------------
664       Trainable params
0         Non-trainable params
664       Total params
0.003     Total estimated model params size (MB)

Epoch 0:   0%|                                                                                                                              | 0/4 [00:00<?, ?it/s]
/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|                                                                                                                              | 0/4 [00:00<?, ?it/s]128
Epoch 0:  25%|█████████                           | 1/4 [00:03<00:10,  3.53s/it, v_num=w3rx, train_generator_loss_step=2.110, train_discriminator_loss_step=0.863]128
Epoch 0:  50%|██████████████████                  | 2/4 [00:03<00:03,  1.77s/it, v_num=w3rx, train_generator_loss_step=1.980, train_discriminator_loss_step=0.849]89
Epoch 0:  75%|███████████████████████████         | 3/4 [00:03<00:01,  1.18s/it, v_num=w3rx, train_generator_loss_step=1.890, train_discriminator_loss_step=0.843]

Epoch 1:   0%| | 0/4 [00:00<?, ?it/s, v_num=w3rx, train_generator_loss_step=1.890, train_discriminator_loss_step=0.843, val_discriminator_loss=1.490, val_generato128
Epoch 1:  25%|▎| 1/4 [00:18<00:55, 18.45s/it, v_num=w3rx, train_generator_loss_step=1.760, train_discriminator_loss_step=0.847, val_discriminator_loss=1.490, val_128
Epoch 1:  50%|▌| 2/4 [00:18<00:18,  9.23s/it, v_num=w3rx, train_generator_loss_step=1.640, train_discriminator_loss_step=0.888, val_discriminator_loss=1.490, val_89
Epoch 1:  75%|▊| 3/4 [00:18<00:06,  6.15s/it, v_num=w3rx, train_generator_loss_step=1.410, train_discriminator_loss_step=0.904, val_discriminator_loss=1.490, val_

Validation: 0it [00:00, ?it/s]
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x127722f70>
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Error in sys.excepthook:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/wandb/sdk/lib/exit_hooks.py", line 42, in exc_handler
    def exc_handler(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 38691) is killed by signal: Interrupt: 2.
Original exception was:
Traceback (most recent call last):
  File "/Users/nehcream/Documents/Workspace/Active/AI-SPARK-Challenge/train.py", line 138, in <module>
    main(config)
  File "/Users/nehcream/Documents/Workspace/Active/AI-SPARK-Challenge/train.py", line 98, in main
    result = trainer.validate(model, val_dataloaders=val_loader)
TypeError: validate() got an unexpected keyword argument 'val_dataloaders'