
Sanity Checking: 0it [00:00, ?it/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name          | Type          | Params
------------------------------------------------
0 | generator     | Generator     | 575
1 | discriminator | Discriminator | 89
------------------------------------------------
664       Trainable params
0         Non-trainable params
664       Total params
0.003     Total estimated model params size (MB)
/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.



Epoch 0:  79%|██████████████████████████▋       | 11/14 [00:11<00:03,  1.04s/it, v_num=nehg, train_generator_loss_step=1.010, train_discriminator_loss_step=1.080]

Validation: 0it [00:00, ?it/s]
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x128c61f70>
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 267, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 187, in advance
    batch = next(data_fetcher)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 184, in __next__
    return self.fetching_function()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 265, in fetching_function
    self._fetch_next_batch(self.dataloader_iter)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py", line 280, in _fetch_next_batch
    batch = next(iterator)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py", line 569, in __next__
    return self.request_next_batch(self.loader_iters)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py", line 581, in request_next_batch
    return apply_to_collection(loader_iters, Iterator, next)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py", line 51, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1359, in _next_data
    idx, data = self._get_data()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1325, in _get_data
    success, data = self._try_get_data()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1163, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
    rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/lightning_utilities/core/rank_zero.py", line 27, in wrapped_fn
    return fn(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/lightning_utilities/core/rank_zero.py", line 64, in rank_zero_warn
    _warn(message, stacklevel=stacklevel, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/lightning_utilities/core/rank_zero.py", line 58, in _warn
    warnings.warn(message, stacklevel=stacklevel, **kwargs)
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/warnings.py", line 403, in __init__
    def __init__(self, message, category, filename, lineno, file=None,
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 40102) is killed by signal: Interrupt: 2.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/Users/nehcream/Documents/Workspace/Active/AI-SPARK-Challenge/train.py", line 139, in <module>
    main(config)
  File "/Users/nehcream/Documents/Workspace/Active/AI-SPARK-Challenge/train.py", line 90, in main
    trainer.fit(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
    rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
  File "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 40162) is killed by signal: Interrupt: 2.